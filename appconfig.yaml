# RAG Configuration File

retriever:
  # Example embedding model configuration with Ollama

  # Available providers: openai, huggingface, ollama, cohere, google_palm, 
  # google_vertex, google_generative_ai, instructor, jina, sentence_transformer, 
  # amazon_bedrock, mistral, voyageai, together_ai, roboflow, text2vec, 
  # openclip, morph, cloudflare_workers_ai, baseten, bm25, huggingface_sparse,
  # fastembed_sparse, chroma_langchain, default.

  provider: ollama

  # Below parameters must match the chosen embedding model provider. You can check them in chromadb documentation
  # https://docs.trychroma.com/docs/embeddings/embedding-functions
  
  url: http://localhost:11434
  model_name: hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF:latest
  #api_key: RETRIEVER_API_KEY # <-- Read from environment variable

LLM:
  model: openai/gpt-oss-20b:nebius
  base_url: https://router.huggingface.co/v1
  api_key: OPENAI_API_KEY # <-- Read from environment variable

vectorstore:
  # TODO
  chroma_client: cloud # local or cloud. Local -> HTTP client, Cloud -> Cloud client
  collection_name: documents
  
  # For local Chroma instance
  host: localhost
  port: 8000

  # For Chroma Cloud instance
  #tenant: TENANT_ID # <-- Optional, Read from environment variable
  database: rag-vectorstore
  api_key: CHROMA_CLOUD_API_KEY # <-- Read from environment variable
